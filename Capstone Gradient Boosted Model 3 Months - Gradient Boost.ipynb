{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8100b996",
   "metadata": {},
   "source": [
    "# Project Geminae MidPoint Model\n",
    "## Gradient Boosted Regression Model for 3 and 6 month projections\n",
    "\n",
    "Tom Gregg\n",
    "\n",
    "2024-02-25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702de5cb",
   "metadata": {},
   "source": [
    "## Setting Up The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b801fc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Basic Libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc0b025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries and Packages to perform Boosted Tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform, randint\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d556367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max Display \n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1c71da",
   "metadata": {},
   "source": [
    "## Importing and Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661c63f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating our file path for the CSV\n",
    "file_path = 'https://raw.githubusercontent.com/tbgregg000/Capstone/main/Cleaned_GenericWellData.csv'\n",
    "df = pd.read_csv(file_path).copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73893240",
   "metadata": {},
   "outputs": [],
   "source": [
    "dff = pd.read_csv(file_path).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac18b467",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef5dae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76be9057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping Columns After Column Index 43 Since Those Are All 9+ Months\n",
    "df = df.iloc[:, :44]  # Select columns up to index 42 (excluding 43)  \n",
    "df.drop(df.columns[26], axis=1, inplace=True)\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aabe1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67915bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data into Water, Gas, and Oil \n",
    "# Splitting data into 3 month and 6 month\n",
    "y_w_3 = df_cleaned['First3MonthWater_BBL']\n",
    "y_g_3 = df_cleaned['First3MonthGas_MCF']\n",
    "y_o_3 = df_cleaned['First3MonthOil_BBL']\n",
    "y_w_6 = df_cleaned['First6MonthWater_BBL']\n",
    "y_g_6 = df_cleaned['First6MonthGas_MCF']\n",
    "y_o_6 = df_cleaned['First6MonthOil_BBL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5703d69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating X using just the non-production columns\n",
    "X = df_cleaned.iloc[:, :26]\n",
    "X = X.drop(\"Well Index\", axis=1)\n",
    "\n",
    "# Date Cleanup\n",
    "columns_to_change = ['InitialProductionDate','DrillingStartDate','DrillingCompletionDate']\n",
    "\n",
    "# Loop through specific columns and rename\n",
    "for col in columns_to_change:\n",
    "    new_name = col + 'Num'\n",
    "    X.rename(columns={col: new_name}, inplace=True)\n",
    "    X[new_name] = X[new_name].apply(lambda x: datetime.strptime(x, \"%Y-%m-%d\").timestamp())\n",
    "\n",
    "\n",
    "# Dropping a few unnecessary columns\n",
    "X = X.drop('InitialProductionMonth', axis = 1)\n",
    "X = X.drop('DrillingCompletionDateNum', axis = 1)\n",
    "X = X.drop('DrillingDuration_DAYS', axis = 1)\n",
    "X = X.drop('ProductionMonthsCount', axis = 1)\n",
    "X = X.drop('YearOfDrilling', axis = 1)\n",
    "X = X.drop('InitialProductionYear', axis = 1)\n",
    "\n",
    "\n",
    "# Dummy Variables for OilTest_Method\n",
    "# Use pd.get_dummies to create dummy variables\n",
    "dummy_vars = pd.get_dummies(X['OilTest_Method'], prefix='OilTest_Method', drop_first=True)\n",
    "\n",
    "# Add the dummy variables as new columns to your DataFrame\n",
    "X = pd.concat([X.drop(\"OilTest_Method\", axis=1), dummy_vars], axis=1)\n",
    "\n",
    "# Converting Objects to Ints\n",
    "for col in X.columns:\n",
    "    if pd.api.types.is_object_dtype(X[col]):\n",
    "        X[col] = X[col].str.replace(',', '')\n",
    "        X[col] = X[col].str.replace(' ', '')\n",
    "        X[col] = X[col].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee972cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c915037b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a52292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the test and train split using seed 99\n",
    "# Quite nice how we can just use the exact same X set\n",
    "X_train, X_test, y_train_w_3, y_test_w_3 = train_test_split(X, y_w_3, test_size=0.2, random_state=99)\n",
    "X_train, X_test, y_train_g_3, y_test_g_3 = train_test_split(X, y_g_3, test_size=0.2, random_state=99)\n",
    "X_train, X_test, y_train_o_3, y_test_o_3 = train_test_split(X, y_o_3, test_size=0.2, random_state=99)\n",
    "\n",
    "X_train, X_test, y_train_w_6, y_test_w_6 = train_test_split(X, y_w_6, test_size=0.2, random_state=99)\n",
    "X_train, X_test, y_train_g_6, y_test_g_6 = train_test_split(X, y_g_6, test_size=0.2, random_state=99)\n",
    "X_train, X_test, y_train_o_6, y_test_o_6 = train_test_split(X, y_o_6, test_size=0.2, random_state=99)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e22d41f",
   "metadata": {},
   "source": [
    "## Boosted Tree Model\n",
    "\n",
    "Scikit-learn reference:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn-ensemble-gradientboostingregressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f597178",
   "metadata": {},
   "source": [
    "### Doing a GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca21f749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the parameter grid\n",
    "# param_grid = {\n",
    "#     'learning_rate': [0.01, 0.75, 0.1, 0.25],\n",
    "#     'n_estimators': [300, 400, 500, 750],\n",
    "#     'max_depth': [5, 7, 9, 11],\n",
    "#     'alpha': [0.1, 0.5, 0.75, 0.999]\n",
    "# }\n",
    "# gb_mod_t = GradientBoostingRegressor(random_state=99)\n",
    "# grid_search = GridSearchCV(estimator=gb_mod_t, param_grid=param_grid, cv = 2, scoring='r2')\n",
    "# # Fit the grid search to your data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fb7bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_search.fit(X_train, y_train_w_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d26f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best model and its parameters\n",
    "# best_model = grid_search.best_estimator_\n",
    "# best_params = grid_search.best_params_\n",
    "\n",
    "# Print the best parameters and score\n",
    "# print(\"Best parameters:\", best_params)\n",
    "# print(\"Best score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e62958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(grid_search.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948b430a",
   "metadata": {},
   "source": [
    "### Doing a Much faster RandomSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45af9cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define distributions for hyperparameters\n",
    "# from scipy.stats import uniform, randint\n",
    "# param_dist = {\n",
    "#     'learning_rate': uniform(0.05, 0.80),\n",
    "#     'n_estimators': randint(300, 1000),\n",
    "#     'max_depth': randint(5, 13),\n",
    "#     'alpha': uniform(0.2, 0.8)\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257c2dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Specify the number of iterations for random search\n",
    "# n_iter_search = 10\n",
    "\n",
    "# # Create the RandomizedSearchCV object\n",
    "# random_search = RandomizedSearchCV(estimator=gb_mod_t, param_distributions=param_dist, n_iter=n_iter_search, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8866967b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random_search.fit(X_train, y_train_w_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d406611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_model = random_search.best_estimator_\n",
    "# best_score = random_search.best_score_\n",
    "\n",
    "# # Print the best parameters and score\n",
    "# print(\"Best parameters:\", best_params)\n",
    "# print(\"Best score:\", best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71b0ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(random_search.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c5c2f7",
   "metadata": {},
   "source": [
    "### We will do Water First"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925fd617",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_mod_0 = GradientBoostingRegressor(learning_rate=0.1, n_estimators= 300, max_depth = 7, random_state=99, alpha = 0.99)\n",
    "gb_mod_0.fit(X_train, y_train_w_3)\n",
    "print(\"Gradient Boost (default parameters) Train R2: \", gb_mod_0.score(X_train, y_train_w_3))\n",
    "print(\"Gradient Boost (default parameters) Test R2: \", gb_mod_0.score(X_test, y_test_w_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed5905b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_mod_1 = GradientBoostingRegressor(learning_rate=0.01, n_estimators= 300, max_depth = 7, random_state=99, alpha = 0.99)\n",
    "gb_mod_1.fit(X_train, y_train_w_3)\n",
    "print(\"Gradient Boost (default parameters) Train R2: \", gb_mod_1.score(X_train, y_train_w_3))\n",
    "print(\"Gradient Boost (default parameters) Test R2: \", gb_mod_1.score(X_test, y_test_w_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0825908",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_mod_2 = GradientBoostingRegressor(learning_rate=1, n_estimators= 300, max_depth = 7, random_state=99, alpha = 0.99)\n",
    "gb_mod_2.fit(X_train, y_train_w_3)\n",
    "print(\"Gradient Boost (default parameters) Train R2: \", gb_mod_2.score(X_train, y_train_w_3))\n",
    "print(\"Gradient Boost (default parameters) Test R2: \", gb_mod_2.score(X_test, y_test_w_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef66b85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_mod_3 = GradientBoostingRegressor(learning_rate=0.1, n_estimators= 300, max_depth = 9, random_state=99, alpha = 0.99)\n",
    "gb_mod_3.fit(X_train, y_train_w_3)\n",
    "print(\"Gradient Boost (default parameters) Train R2: \", gb_mod_3.score(X_train, y_train_w_3))\n",
    "print(\"Gradient Boost (default parameters) Test R2: \", gb_mod_3.score(X_test, y_test_w_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345b2c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_mod_4 = GradientBoostingRegressor(learning_rate=0.075, n_estimators= 500, max_depth = 9, random_state=99, alpha = 0.99)\n",
    "gb_mod_4.fit(X_train, y_train_w_3)\n",
    "print(\"Gradient Boost (default parameters) Train R2: \", gb_mod_4.score(X_train, y_train_w_3))\n",
    "print(\"Gradient Boost (default parameters) Test R2: \", gb_mod_4.score(X_test, y_test_w_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d3bcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_mod_5 = GradientBoostingRegressor(learning_rate=0.075, n_estimators= 500, max_depth = 9, random_state=99, alpha = 0.99)\n",
    "gb_mod_5.fit(X_train, y_train_w_3)\n",
    "print(\"Gradient Boost (default parameters) Train R2: \", gb_mod_5.score(X_train, y_train_w_3))\n",
    "print(\"Gradient Boost (default parameters) Test R2: \", gb_mod_5.score(X_test, y_test_w_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf08ce57",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_mod_6 = GradientBoostingRegressor(learning_rate=0.1, n_estimators= 50, max_depth = 8, random_state=99, alpha = 0.99)\n",
    "gb_mod_6.fit(X_train, y_train_w_3)\n",
    "print(\"Gradient Boost (default parameters) Train R2: \", gb_mod_6.score(X_train, y_train_w_3))\n",
    "print(\"Gradient Boost (default parameters) Test R2: \", gb_mod_6.score(X_test, y_test_w_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b261283",
   "metadata": {},
   "source": [
    "### Fucking Oil Man"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6763f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_mod_7 = GradientBoostingRegressor(learning_rate=0.075, n_estimators= 400, max_depth = 7, random_state=99, alpha = 0.5)\n",
    "gb_mod_7.fit(X_train, y_train_o_3)\n",
    "print(\"Gradient Boost (default parameters) Train R2: \", gb_mod_7.score(X_train, y_train_o_3))\n",
    "print(\"Gradient Boost (default parameters) Test R2: \", gb_mod_7.score(X_test, y_test_o_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c944c029",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_mod_8 = GradientBoostingRegressor(learning_rate=0.075, n_estimators= 500, max_depth = 10, random_state=99, alpha = 0.5)\n",
    "gb_mod_8.fit(X_train, y_train_o_3)\n",
    "print(\"Gradient Boost (default parameters) Train R2: \", gb_mod_8.score(X_train, y_train_o_3))\n",
    "print(\"Gradient Boost (default parameters) Test R2: \", gb_mod_8.score(X_test, y_test_o_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b189658",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_mod_9 = GradientBoostingRegressor(learning_rate=0.1, n_estimators= 300, max_depth = 8, random_state=99, alpha = 0.5)\n",
    "gb_mod_9.fit(X_train, y_train_o_3)\n",
    "print(\"Gradient Boost (default parameters) Train R2: \", gb_mod_9.score(X_train, y_train_o_3))\n",
    "print(\"Gradient Boost (default parameters) Test R2: \", gb_mod_9.score(X_test, y_test_o_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d9a6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_mod_10 = GradientBoostingRegressor(learning_rate=0.1, n_estimators= 500, max_depth = 10, random_state=99, alpha = 0.5)\n",
    "gb_mod_10.fit(X_train, y_train_o_3)\n",
    "print(\"Gradient Boost (default parameters) Train R2: \", gb_mod_10.score(X_train, y_train_o_3))\n",
    "print(\"Gradient Boost (default parameters) Test R2: \", gb_mod_10.score(X_test, y_test_o_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24ae141",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_mod_11 = GradientBoostingRegressor(learning_rate=0.075, n_estimators= 400, max_depth = 10, random_state=99, alpha = 0.5)\n",
    "gb_mod_11.fit(X_train, y_train_o_3)\n",
    "print(\"Gradient Boost (default parameters) Train R2: \", gb_mod_11.score(X_train, y_train_o_3))\n",
    "print(\"Gradient Boost (default parameters) Test R2: \", gb_mod_11.score(X_test, y_test_o_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa2617d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "y_pred = gb_mod_11.predict(X_test)\n",
    "y_test = y_test_o_3\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Absolute Error (MAE): {round(mae,2)}\")\n",
    "print(f\"Mean Squared Error (MSE): {round(mse,2)}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {round(rmse,2)}\")\n",
    "print(f\"R-squared (R²): {round(r2,6)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f42433",
   "metadata": {},
   "source": [
    "## Let's make some fucking charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f7027d",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = X_train.columns\n",
    "# Extract feature importances from the model\n",
    "importances = gb_mod_11.feature_importances_\n",
    "# Sort features and importances in descending order of importance\n",
    "sorted_idx = importances.argsort()[::-1]\n",
    "sorted_names = [feature_names[i] for i in sorted_idx][::-1]\n",
    "sorted_importances = importances[sorted_idx][::-1]\n",
    "\n",
    "# Create the bar plot\n",
    "plt.figure(figsize=(10, 6))  # Adjust figure size as needed\n",
    "plt.barh(sorted_names, sorted_importances)\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.ylabel('Feature Name')\n",
    "plt.title('Feature Importance for Gradient Boosting Model')\n",
    "plt.xticks(rotation=45, ha='right', fontsize = 8)  # Rotate feature names for better readability\n",
    "plt.yticks(fontsize = 8)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c2cd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "\n",
    "# Choose the tree index to visualize (between 0 and number of trees - 1)\n",
    "tree_index = 4  # Change this to the desired tree index\n",
    "\n",
    "# Extract the tree object from the model\n",
    "tree = gb_mod_5.estimators_[tree_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4161db50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "export_graphviz(\n",
    "        gb_mod_5,\n",
    "        out_file=\"tree.dot\",\n",
    "        feature_names=X_train.columns,\n",
    "        impurity=False,\n",
    "        rounded=True,\n",
    "        filled=True\n",
    "    )\n",
    "Source.from_file(\"tree.dot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafa07dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dff.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e09cf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c47131a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data (modify with your actual data)\n",
    "var1 = dff['TrueVerticalDepth_FT']\n",
    "var2 = dff['MeasuredDepth_FT']\n",
    "\n",
    "# Create the plot\n",
    "plt.hist(var1, bins='auto', alpha=0.5, label='Vertical Depth')\n",
    "plt.hist(var2, bins='auto', alpha=0.5, label='Full Measured Length')\n",
    "plt.xlabel('Feet')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Frequency Distribution of Well Depth')\n",
    "plt.legend()\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb0cfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data (modify with your actual data)\n",
    "var1 = dff['CumOil_BBL']\n",
    "\n",
    "# Create the plot\n",
    "plt.hist(var1, bins='auto', alpha=0.5)\n",
    "plt.xlabel('Barrels of Oil')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Frequency Distribution of Oil Production in Barrels')\n",
    "plt.legend()\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c546d02",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Sample data (modify with your actual data)\n",
    "var1 = dff['ProductionMonthsCount']\n",
    "\n",
    "# Create the plot\n",
    "plt.hist(var1, bins='auto', alpha=0.5)\n",
    "plt.xlabel('Number of Months')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Frequency Distribution of Production Timeline per Well')\n",
    "plt.legend()\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63865c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the bar plot\n",
    "# new imports\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "std_x_train = X_train.copy()\n",
    "std_x_test = X_test.copy()\n",
    "\n",
    "std_train_array = scaler.transform(std_x_train)\n",
    "std_test_array = scaler.transform(std_x_test)\n",
    "\n",
    "std_x_train[:] = std_train_array\n",
    "std_x_test[:] = std_test_array\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=len(X_train.columns))\n",
    "pca.fit(std_x_train[:])\n",
    "\n",
    "\n",
    "# Example data: Explained variance ratio for each principal component\n",
    "explained_variance_ratio = np.array(pca.explained_variance_ratio_)\n",
    "\n",
    "# Cumulative explained variance\n",
    "cumulative_explained_variance = pca.explained_variance_ratio_.cumsum()\n",
    "\n",
    "# Number of components\n",
    "components = range(1, len(explained_variance_ratio) + 1)\n",
    "\n",
    "# Creating the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(components, explained_variance_ratio, alpha=0.5, label='Individual explained variance')\n",
    "plt.plot(components, cumulative_explained_variance, marker='o', linestyle='-', color='r', label='Cumulative explained variance')\n",
    "\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.title('PCA Explained Variance')\n",
    "plt.xticks(components, X_train.columns[:pca.n_components_], rotation=45, fontsize = 8, ha='right')\n",
    "plt.legend(loc='best')\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
